{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a04ed3-6c21-4064-8199-64f324d704ba",
   "metadata": {},
   "source": [
    "# Model View Projection\n",
    "\n",
    "A model (either a triangle mesh, a signed distance function, etc) is specified with respect to a _local frame_. When building a scene, the local frame of each object must be transform into a _world frame_. The world in computer graphics is seen through a camera, so each object in the world frame, must be transform to a _view frame_ that gives its position with respect to the camera. We are not done yet, a (pinhole) camera can see in a 2D square, so we need to _project_ the view frame into a 2D plane (called the _image plane_). The projection can be choosen to be:\n",
    "* orthographic: keep parallel lines at infinity.\n",
    "* perspective: merge parallel lines at infinity.\n",
    "\n",
    "```\n",
    "    Local frame  -->  World frame  -->  View frame   -->  Image plane\n",
    "```\n",
    "\n",
    "Let\n",
    "* $M\\colon \\mathbb R^3 \\to \\mathbb R^3$ be the transformation from the local frame to the world frame,\n",
    "* $V\\colon \\mathbb R^3 \\to \\mathbb R^3$ be the transformation from the world frame to the view frame and\n",
    "* $P\\colon\\mathbb R^3 \\to \\mathbb R^2$ be the transformation of the view frame to the image plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb990cf-da31-4cbf-ba81-f6ee74a2d054",
   "metadata": {},
   "source": [
    "## Model Matrix\n",
    "\n",
    "The movement of local frame to world frame are rigid transformation (scaling, reflection and translation). Then, the function $M$ adopts the form\n",
    "$$\n",
    "    M(\\mathbf x) = \\mathbf o + \\mathbf S \\mathbf x\n",
    "$$\n",
    "where $\\mathbf o$ is the translation vector and $\\mathbf S$ is are the other transformations. Now, we will apply a trick that is often used in computers graphics. Since most computations run on the GPU, we want every transformation to look like matrix multiplication. To achieve this, its convenient to modify the local coordinates as to append a $1$ at the end i.e. $\\mathbf x \\leftarrow (\\mathbf x, 1)\\in\\mathbb R^3$. Then, the transformation $M$ can be writen with a matrix $\\mathbf M \\in \\mathbb R^{4\\times 4}$ as:\n",
    "$$\n",
    "    \\mathbf M = \n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf S & \\mathbf o\\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "where the first 3 coordinates in the image are the model coordinates and the last one is a mousketool for latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aed479f-8725-4754-86e2-eead27fd7906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8010b27a-730b-4972-97d5-8913a4c05c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local frame [1 1 0 1]\n",
      "Model frame [0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 0, 1])\n",
    "\n",
    "M = np.array([\n",
    "    [1, 0, 0, -1],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "print('Local frame', x)\n",
    "print('Model frame', M@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e05a25-03e9-4839-8dc3-2c29b676a12e",
   "metadata": {},
   "source": [
    "## View Matrix\n",
    "\n",
    "In this step, world coordinates must be expressed from the camera view point. Let $\\mathbf c_O, \\mathbf c_F, \\mathbf c_R$ and $\\mathbf c_U$ be vectors in $\\mathbb R^3$ representing the camera origin, the camera front direction, the camera right direction and the camera upward direction such that $\\mathbf c_R = \\mathbf c_F \\times \\mathbf c_U$.\n",
    "\n",
    "Let $\\mathbf x \\in \\mathbb R^3$. Then, $\\mathbf x' = \\mathbf x - \\mathbf c_O$ is the vector $\\mathbf x$ with the respect to the origin $\\mathbf c_O$. Then, we want to find coefficients $y_1, y_2$ and $y_3$ such that \n",
    "$$\n",
    "    y_1 \\mathbf c_F + y_2 \\mathbf c_R + y_3 \\mathbf c_U = \\mathbf x'.\n",
    "$$\n",
    "Let $\\mathbf C = \\begin{bmatrix} c_F & c_R & c_U \\end{bmatrix}$ and $\\mathbf y = (y_1, y_2, y_3)$. Then,\n",
    "$$\n",
    "    \\mathbf C \\mathbf y = \\mathbf x'\n",
    "    \\iff\n",
    "    \\mathbf y = \\mathbf C^{-1} \\mathbf x'.\n",
    "$$\n",
    "Therefore, the transformation can be writen as:\n",
    "$$\n",
    "    V(\\mathbf x) = \\mathbf C^{-1}(\\mathbf x - \\mathbf c_O)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927ad2ca-0cec-4109-870f-76691972342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera information\n",
      "Origin : [1 0 1]\n",
      "Front  : [ 0  0 -1]\n",
      "Upward : [0 1 0]\n",
      "Right  : [1 0 0]\n",
      "Matrix :\n",
      "[[ 0  0  1]\n",
      " [ 0  1  0]\n",
      " [-1  0  0]]\n",
      "Old coord: [1 1 0]\n",
      "New coord: [1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 0])\n",
    "\n",
    "cO = np.array([1, 0, 1])\n",
    "cF = np.array([0, 0, -1])\n",
    "cU = np.array([0, 1, 0])\n",
    "cR = np.cross(cF, cU)\n",
    "\n",
    "print('Camera information')\n",
    "print('Origin :', cO)\n",
    "print('Front  :', cF)\n",
    "print('Upward :', cU)\n",
    "print('Right  :', cR)\n",
    "\n",
    "C = np.array([cF, cU, cR]).T\n",
    "print('Matrix :')\n",
    "print(np.matrix(C))\n",
    "\n",
    "print('Old coord:', x)\n",
    "print('New coord:', np.linalg.solve(C, (x-cO)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da728fc-7578-4c9c-b824-e459d6487e8a",
   "metadata": {},
   "source": [
    "As a matrix transformation, we append a one a consider the 4 by 4 matrix:\n",
    "$$\n",
    "    \\mathbf V = \\begin{bmatrix} \\mathbf C^{-1} & -\\mathbf C^{-1} \\mathbf c_O\\\\ 0 & 1\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bbfac-d88d-4ce2-810b-0e7fc8912eab",
   "metadata": {},
   "source": [
    "## Projection Matrix\n",
    "\n",
    "We are one step away of finally drawing something on screen. Recall that projection matrices\n",
    "come in two flavors: orthographic and perspective.\n",
    "\n",
    "**Perspective Projection** Consider a truncated pyramid coming out of the camera. Everything inside of that truncated pyramid will appear on screen. The truncated pyramid can be define by a number of parameters, in this section we consider the following parameterization:\n",
    "* $n$ be the near distance plane (objects closer that $n$ to the camera will not appear on screen),\n",
    "* $f$ be the far distance plane (objects farther that $f$ to the camera will not appear on screen),\n",
    "* $\\alpha$ be the vertical field of view of the camera,\n",
    "* $\\beta$ be the horizontal field of view of the camera.\n",
    "\n",
    "\n",
    "We first map the truncated pyramid (or view frustrum) defined by the above parameters into the unit cube $[-1,1]^{3}$ called the _image space_ (two dimensions for the objects location and the third one is a hint for the rasterizer).\n",
    "Consider a slice of the view frustrum (which is a parallelepid) at depth $d$, where depth is measured from the camera origin. Let $h$ and $v$ be the length of the horizontal and vertical sides measure from the frustum center. Then, the map $(r_h,r_v,r_d) \\to (h,v,d)$ from image space to the view frustum is given by:\n",
    "$$\n",
    "    v = r_v \\tan(\\alpha) d\n",
    "    \\qquad\n",
    "    h = r_h \\tan(\\beta) d\n",
    "    \\qquad\n",
    "    d = \\frac{n+f}{2} + r_d \\frac{f-n}{2}\n",
    "$$\n",
    "Computing the inverse gives the map from the view frustrum to image space.\n",
    "$$\n",
    "    \\varphi(h,v,d) \n",
    "    =\n",
    "    \\left(\n",
    "        h\\, \\frac{\\cot \\beta}{d},\n",
    "        y\\, \\frac{\\cot \\alpha}{d},\n",
    "        d\\, \\frac{2}{f-n} - \\frac{f+n}{f-n}\n",
    "    \\right).\n",
    "$$\n",
    "Note that $\\varphi$ is not a linear function, so cannot write in a matrix form. Nevertheless, we consider the following \"linearization trick\":\n",
    "$$\n",
    "    \\hat\\varphi(h,v,d,1) =\n",
    "    \\left(\n",
    "        h\\, \\cot \\beta,\n",
    "        y\\, \\cot \\alpha,\n",
    "        \\frac{2}{f-n} - d \\frac{f+n}{f-n},\n",
    "        d\n",
    "    \\right),\n",
    "$$\n",
    "which can be writen with the following matrix:\n",
    "$$\n",
    "    \\mathbf P \n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        \\cot \\beta & 0 & 0 & 0\\\\\n",
    "        0 & \\cot\\alpha & 0 & 0\\\\\n",
    "        0 & 0 & -\\frac{f+n}{f-n} & \\frac{2}{f-n}\\\\\n",
    "        0 & 0 & 1 & 0\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "then, the coordinates in image space can be recovered by dividing the first three coordinates by the fourth one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdff0c-9dfe-4c04-adc9-d19ab3f46a8b",
   "metadata": {},
   "source": [
    "### Finally, a working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8a5e12-777a-4c7a-b4f3-0123bfedef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed cube (in 4d)\n",
      "[[-0.176 -0.176  0.176  0.176 -0.176 -0.176  0.176  0.176]\n",
      " [-0.176  0.176  0.176 -0.176 -0.176  0.176  0.176 -0.176]\n",
      " [-3.286 -3.286 -3.286 -3.286 -0.714 -0.714 -0.714 -0.714]\n",
      " [ 3.     3.     3.     3.     1.     1.     1.     1.   ]]\n",
      "Perspective cube\n",
      "[[-0.059 -0.059  0.059  0.059 -0.176 -0.176  0.176  0.176]\n",
      " [-0.059  0.059  0.059 -0.059 -0.176  0.176  0.176 -0.176]\n",
      " [-1.095 -1.095 -1.095 -1.095 -0.714 -0.714 -0.714 -0.714]]\n"
     ]
    }
   ],
   "source": [
    "'''  Camera information  '''\n",
    "cO = np.array([0, 0, 2])\n",
    "cF = np.array([0, 0, -1])\n",
    "cU = np.array([0, 1, 0])\n",
    "cR = np.cross(cF, cU)\n",
    "\n",
    "C = np.array([cR, cU, cF]).T\n",
    "\n",
    "n = 0.5\n",
    "f = 4\n",
    "alpha = 80 * np.pi/180\n",
    "beta = 80 * np.pi/180\n",
    "\n",
    "'''  Model, View, Projection  '''\n",
    "\n",
    "M = np.eye(4)\n",
    "\n",
    "V = np.block([\n",
    "    [np.linalg.inv(C), -np.linalg.solve(C, cO).reshape((3, 1))],\n",
    "    [np.zeros(3), 1]\n",
    "])\n",
    "\n",
    "P = np.array([\n",
    "    [np.divide(1, np.tan(beta)), 0, 0, 0],\n",
    "    [0, np.divide(1, np.tan(alpha)), 0, 0],\n",
    "    [0, 0, -(f+n)/(f-n), 2/(f-n)],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "\n",
    "'''  Geometry information  '''\n",
    "cube = np.array([\n",
    "    #  Back\n",
    "    [-1, -1, -1, 1],\n",
    "    [-1, 1, -1, 1],\n",
    "    [1, 1, -1, 1],\n",
    "    [1, -1, -1, 1],\n",
    "    #  Front\n",
    "    [-1, -1, 1, 1],\n",
    "    [-1, 1, 1, 1],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, -1, 1, 1],\n",
    "]).T\n",
    "\n",
    "''' Applying transfomation '''\n",
    "cube_4d = P @ V @ M @ cube\n",
    "print('Transformed cube (in 4d)')\n",
    "np.set_printoptions(3)\n",
    "print(cube_4d)\n",
    "\n",
    "cube_img_space = cube_4d[:3] / cube_4d[-1]\n",
    "print('Perspective cube')\n",
    "print(cube_img_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0b3a0-3325-4f72-8b9b-c2fab8c74c77",
   "metadata": {},
   "source": [
    "We can see reading each vector that the back face is closed to the origin than the front face (1 order of magnitude, 0.059 vs 0.176). Since numbers are not so satisfying to read, let's build an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc34cc7-5431-45d6-bde3-73c411a33910",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (200, 200)\n",
    "\n",
    "cube_coords = img_size[0]//2 + np.multiply(cube_img_space[:2], img_size[0]).astype(int)\n",
    "\n",
    "depth = (cube_img_space[2].clip(min=-1.0, max=1.0) + 2.0)  #  a random proportionality function\n",
    "\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "canvas = Canvas(width=img_size[0], height=img_size[1], sync_image_data=True)\n",
    "canvas.fill_rects(cube_coords[0], cube_coords[1], 5.0 * depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ba4e3f-8f78-474d-b256-532b52890953",
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas.to_file('images/mvp_cube.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4ed94-5d1f-4597-820a-640982fac3bb",
   "metadata": {},
   "source": [
    "<img src=\"images/mvp_cube.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131a963-c5ad-4a23-a9be-fc50e9d42e58",
   "metadata": {},
   "source": [
    "Note that we did a depth deformation using the 3rd coordinate of the cube in image space. Now, let's build something more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350a0ec2-3c87-4262-93b3-5c32301e5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Nodes  : (4, 1845)\n",
      "# Faces  : (3, 3686)\n"
     ]
    }
   ],
   "source": [
    "''' We will use pyvista to load the data, it's not hard doing it by hand but that is not the purpose here '''\n",
    "\n",
    "import pyvista as pv\n",
    "\n",
    "MAX_FACES = 15_000\n",
    "\n",
    "#  Mesh from https://www.thingiverse.com/thing:6703649/files\n",
    "mesh = pv.PolyData('resources/3D_models/blue_whale/files/ballena_azul_Lowpoly.stl')\n",
    "\n",
    "num_faces = len(mesh.faces) // 4\n",
    "\n",
    "if num_faces > MAX_FACES:\n",
    "    mesh = mesh.decimate(1.0 - (MAX_FACES / num_faces))\n",
    "    print('Decimated mesh, from ', num_faces, 'faces to', len(mesh.faces)//4, 'faces')\n",
    "\n",
    "n_nodes = mesh.points.shape[0]\n",
    "nodes = np.concatenate((mesh.points, np.ones((n_nodes, 1))), axis=1).T\n",
    "triangles = np.delete(mesh.faces.reshape(-1, 4), 0, 1).T\n",
    "\n",
    "print('# Nodes  :', nodes.shape)\n",
    "print('# Faces  :', triangles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e74a774-de8b-40be-96c3-33063672167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Camera information  '''\n",
    "camera_target = np.mean(nodes[:3], axis=1)\n",
    "\n",
    "min_1, max_1 = np.amin(nodes[0]), np.amax(nodes[0])\n",
    "min_2, max_2 = np.amin(nodes[1]), np.amax(nodes[1])\n",
    "min_3, max_3 = np.amin(nodes[2]), np.amax(nodes[2])\n",
    "\n",
    "cO = 2 * np.array([3 * max_1, 3 * max_2, 0.5 * (max_3 + min_3)])\n",
    "cF = cO - camera_target\n",
    "cF /= np.linalg.norm(cF)\n",
    "cU = -np.array([0, 1, 0])\n",
    "cR = np.cross(cF, cU)\n",
    "\n",
    "C = np.array([cR, cU, cF]).T  #  x (horizontal), y (vertical), z (depth) as seem from the camera \n",
    "\n",
    "n = 10\n",
    "f = 100\n",
    "alpha = 30.0 * np.pi/180\n",
    "beta = 30.0 * np.pi/180\n",
    "\n",
    "'''  Model=id, View, Projection  '''\n",
    "V = np.block([\n",
    "    [np.linalg.inv(C), -np.linalg.solve(C, cO).reshape((3, 1))],\n",
    "    [np.zeros(3), 1]\n",
    "])\n",
    "\n",
    "P = np.array([\n",
    "    [np.divide(1, np.tan(beta)), 0, 0, 0],\n",
    "    [0, np.divide(1, np.tan(alpha)), 0, 0],\n",
    "    [0, 0, -(f+n)/(f-n), 2/(f-n)],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "''' Draw time '''\n",
    "img_size = (500, 500)\n",
    "\n",
    "img_4d = P @ V @ nodes\n",
    "img_space = img_4d[:3] / img_4d[-1]\n",
    "\n",
    "depth = (img_space[2].clip(min=-1.0, max=1.0) + 2.0)\n",
    "\n",
    "coords = img_size[0]//2 + np.multiply(img_space[:2], img_size[0]).astype(int)\n",
    "mean = np.mean(coords, axis=0)\n",
    "\n",
    "plt = Canvas(width=img_size[0], height=img_size[1], sync_image_data=True)\n",
    "\n",
    "plt.fill_rects(coords[0], coords[1], 1.5 * depth)\n",
    "\n",
    "plt.fill_style = 'red'\n",
    "plt.fill_rect(mean[0], mean[1], 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3bb6b08-14d2-4e6f-bc5f-7feccfad84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.to_file('images/mvp_final.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71edb33-0cde-4646-91e5-42ff00bc4e9d",
   "metadata": {},
   "source": [
    "<img src=\"images/mvp_final.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7f7b3-d28c-4167-a13f-3296ca0dcad5",
   "metadata": {},
   "source": [
    "References:\n",
    "* UC Davis (2015). The Camera Transform. Available at: https://youtu.be/mpTl003EXCY?si=Y7s9tljaW3w3VYuO  (Accessed: 9 May 2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ef32b-d346-4be3-a80a-3906b3b0f35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
